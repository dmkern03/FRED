{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fc37f9b-1e18-4605-8d89-41409549eb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FRED API Rate Data Fetcher - Databricks Edition\n",
    "Pulls rate data for multiple indices and saves to Unity Catalog Volumes.\n",
    "\n",
    "Author: Dave\n",
    "Date: December 2024\n",
    "\n",
    "================================================================================\n",
    "DATABRICKS SETUP INSTRUCTIONS\n",
    "================================================================================\n",
    "\n",
    "1. CREATE UNITY CATALOG VOLUMES AND BRONZE TABLES\n",
    "   -----------------------------------------------\n",
    "   Run this SQL in a Databricks notebook or SQL editor:\n",
    "   \n",
    "   -- Create schema (if needed)  \n",
    "   CREATE SCHEMA IF NOT EXISTS investments.fred;\n",
    "   \n",
    "   -- Create volume for rate observation data\n",
    "   CREATE VOLUME IF NOT EXISTS investments.fred.rates;\n",
    "   \n",
    "   -- Create volume for series metadata/dimensional info\n",
    "   CREATE VOLUME IF NOT EXISTS investments.fred.metadata;\n",
    "   \n",
    "   -- Create Bronze layer tables (empty schemas - data loaded by script)\n",
    "   CREATE TABLE IF NOT EXISTS investments.fred.bronze_rates (\n",
    "       run_timestamp TIMESTAMP,\n",
    "       series_id STRING,\n",
    "       series_name STRING,\n",
    "       date DATE,\n",
    "       value DOUBLE,\n",
    "       ingestion_timestamp TIMESTAMP\n",
    "   )\n",
    "   USING DELTA\n",
    "   COMMENT 'Bronze layer: Raw FRED rate observations';\n",
    "   \n",
    "   CREATE TABLE IF NOT EXISTS investments.fred.bronze_metadata (\n",
    "       run_timestamp TIMESTAMP,\n",
    "       series_id STRING,\n",
    "       friendly_name STRING,\n",
    "       title STRING,\n",
    "       frequency STRING,\n",
    "       frequency_short STRING,\n",
    "       units STRING,\n",
    "       units_short STRING,\n",
    "       seasonal_adjustment STRING,\n",
    "       seasonal_adjustment_short STRING,\n",
    "       observation_start DATE,\n",
    "       observation_end DATE,\n",
    "       last_updated STRING,\n",
    "       popularity INT,\n",
    "       notes STRING,\n",
    "       ingestion_timestamp TIMESTAMP\n",
    "   )\n",
    "   USING DELTA\n",
    "   COMMENT 'Bronze layer: Raw FRED series metadata/dimensional info';\n",
    "\n",
    "2. STORE YOUR FRED API KEY AS A DATABRICKS SECRET\n",
    "   -----------------------------------------------\n",
    "   Using Databricks CLI:\n",
    "   \n",
    "   # Create a secret scope (one-time setup)\n",
    "   databricks secrets create-scope --scope fred-api\n",
    "   \n",
    "   # Store your API key\n",
    "   databricks secrets put --scope fred-api --key api-key\n",
    "   \n",
    "   Or via the Databricks UI:\n",
    "   - Go to your workspace URL + #secrets/createScope\n",
    "   - Create scope named \"fred-api\"\n",
    "   - Add secret named \"api-key\" with your FRED API key value\n",
    "   \n",
    "   Get your free FRED API key at: https://fred.stlouisfed.org/docs/api/api_key.html\n",
    "\n",
    "3. CREATE A DATABRICKS NOTEBOOK\n",
    "   ----------------------------\n",
    "   - Create a new Python notebook in your workspace\n",
    "   - Copy this entire script into the notebook\n",
    "   - Update the CONFIGURATION section below with your catalog/schema/volume names\n",
    "\n",
    "4. SCHEDULE THE NOTEBOOK AS A JOB\n",
    "   ------------------------------\n",
    "   Option A: Via Databricks UI\n",
    "   - Click \"Schedule\" button in the notebook toolbar\n",
    "   - Or go to Workflows > Jobs > Create Job\n",
    "   - Select your notebook as the task\n",
    "   - Set schedule (e.g., daily at 6:00 AM ET)\n",
    "   - Configure cluster (single node is fine for this workload)\n",
    "   - Set alerts/notifications as desired\n",
    "   \n",
    "   Option B: Via Databricks CLI or API\n",
    "   See example JSON config at the bottom of this script.\n",
    "\n",
    "5. RUN THE NOTEBOOK\n",
    "   -----------------\n",
    "   - Run manually to test, or let the schedule trigger it\n",
    "   - Check the volumes for output CSV files:\n",
    "     SELECT * FROM LIST('/Volumes/investments/fred/rates/')\n",
    "     SELECT * FROM LIST('/Volumes/investments/fred/metadata/')\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATABRICKS CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Unity Catalog location\n",
    "CATALOG = \"investments\"\n",
    "SCHEMA = \"fred\"\n",
    "\n",
    "# Separate volumes for different data types (landing zone)\n",
    "VOLUME_RATES = \"rates\"             # For rate observation data\n",
    "VOLUME_METADATA = \"metadata\"       # For series metadata/dimensional info\n",
    "\n",
    "# Construct volume paths (Databricks Volumes path format)\n",
    "VOLUME_PATH_RATES = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_RATES}\"\n",
    "VOLUME_PATH_METADATA = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_METADATA}\"\n",
    "\n",
    "# Secret scope and key name for FRED API key\n",
    "SECRET_SCOPE = \"fred-api\"\n",
    "SECRET_KEY = \"api-key\"\n",
    "\n",
    "# =============================================================================\n",
    "# FRED API CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Base URL for FRED API\n",
    "FRED_BASE_URL = \"https://api.stlouisfed.org/fred\"\n",
    "\n",
    "# Rate series to pull - ADD NEW SERIES HERE\n",
    "# Format: \"SERIES_ID\": \"Friendly Name\"\n",
    "RATE_SERIES = {\n",
    "    # Treasury Rates\n",
    "    \"DFF\": \"Federal Funds Effective Rate\",\n",
    "    \"DTB3\": \"3-Month Treasury Bill Rate\",\n",
    "    \"DGS1\": \"1-Year Treasury Rate\",\n",
    "    \"DGS2\": \"2-Year Treasury Rate\",\n",
    "    \"DGS5\": \"5-Year Treasury Rate\",\n",
    "    \"DGS10\": \"10-Year Treasury Rate\",\n",
    "    \"DGS30\": \"30-Year Treasury Rate\",\n",
    "    \n",
    "    # LIBOR/SOFR Rates\n",
    "    \"SOFR\": \"Secured Overnight Financing Rate (SOFR)\",\n",
    "    \n",
    "    # Prime Rate\n",
    "    \"DPRIME\": \"Bank Prime Loan Rate\",\n",
    "    \n",
    "    # Mortgage Rates\n",
    "    \"MORTGAGE30US\": \"30-Year Fixed Rate Mortgage Average\",\n",
    "    \"MORTGAGE15US\": \"15-Year Fixed Rate Mortgage Average\",\n",
    "    \n",
    "    # Corporate Bond Rates\n",
    "    \"BAMLC0A0CM\": \"ICE BofA US Corporate Index Effective Yield\",\n",
    "    \"BAMLH0A0HYM2\": \"ICE BofA US High Yield Index Effective Yield\",\n",
    "    \n",
    "    # Inflation Expectations\n",
    "    \"T10YIE\": \"10-Year Breakeven Inflation Rate\",\n",
    "    \"T5YIE\": \"5-Year Breakeven Inflation Rate\",\n",
    "}\n",
    "\n",
    "# Optional: Filter by date range (set to None for all available data)\n",
    "OBSERVATION_START = None  # e.g., \"2020-01-01\"\n",
    "OBSERVATION_END = None    # e.g., \"2024-12-31\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# API FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_series_info(series_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get metadata about a FRED series.\n",
    "    \n",
    "    Args:\n",
    "        series_id: The FRED series identifier\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with series metadata\n",
    "    \"\"\"\n",
    "    url = f\"{FRED_BASE_URL}/series\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"api_key\": FRED_API_KEY,\n",
    "        \"file_type\": \"json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    if \"seriess\" in data and len(data[\"seriess\"]) > 0:\n",
    "        return data[\"seriess\"][0]\n",
    "    return {}\n",
    "\n",
    "\n",
    "def fetch_series_metadata(series_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch dimensional/metadata information for multiple series.\n",
    "    \n",
    "    Args:\n",
    "        series_dict: Dictionary mapping series_id to friendly name\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with metadata for all series\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "    run_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    print(f\"\\nFetching metadata for {len(series_dict)} series...\")\n",
    "    \n",
    "    for series_id, friendly_name in series_dict.items():\n",
    "        print(f\"  Fetching metadata for {series_id}...\")\n",
    "        \n",
    "        try:\n",
    "            info = get_series_info(series_id)\n",
    "            \n",
    "            if info:\n",
    "                # Clean notes field - remove line breaks that break CSV parsing\n",
    "                notes = info.get(\"notes\", \"\")\n",
    "                if notes:\n",
    "                    # Replace newlines, carriage returns, and multiple spaces\n",
    "                    notes = notes.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "                    # Remove multiple consecutive spaces\n",
    "                    while \"  \" in notes:\n",
    "                        notes = notes.replace(\"  \", \" \")\n",
    "                    notes = notes.strip()\n",
    "                \n",
    "                # Extract key dimensional information - ALL fields as strings\n",
    "                metadata = {\n",
    "                    \"run_timestamp\": run_timestamp,\n",
    "                    \"series_id\": info.get(\"id\", series_id),\n",
    "                    \"friendly_name\": friendly_name,\n",
    "                    \"title\": info.get(\"title\", \"\"),\n",
    "                    \"frequency\": info.get(\"frequency\", \"\"),\n",
    "                    \"frequency_short\": info.get(\"frequency_short\", \"\"),\n",
    "                    \"units\": info.get(\"units\", \"\"),\n",
    "                    \"units_short\": info.get(\"units_short\", \"\"),\n",
    "                    \"seasonal_adjustment\": info.get(\"seasonal_adjustment\", \"\"),\n",
    "                    \"seasonal_adjustment_short\": info.get(\"seasonal_adjustment_short\", \"\"),\n",
    "                    \"observation_start\": info.get(\"observation_start\", \"\"),\n",
    "                    \"observation_end\": info.get(\"observation_end\", \"\"),\n",
    "                    \"last_updated\": info.get(\"last_updated\", \"\"),\n",
    "                    \"popularity\": str(info.get(\"popularity\", \"\")),\n",
    "                    \"notes\": notes\n",
    "                }\n",
    "                all_metadata.append(metadata)\n",
    "                print(f\"    ✓ Retrieved metadata\")\n",
    "            else:\n",
    "                print(f\"    ⚠ No metadata available\")\n",
    "                \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"    ✗ Error: {e}\")\n",
    "        \n",
    "        # Be nice to the API - small delay between requests\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    if all_metadata:\n",
    "        return pd.DataFrame(all_metadata)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def get_series_observations(\n",
    "    series_id: str,\n",
    "    observation_start: str = None,\n",
    "    observation_end: str = None,\n",
    "    frequency: str = None,\n",
    "    sort_order: str = \"asc\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get observations (data values) for a FRED series.\n",
    "    \n",
    "    Args:\n",
    "        series_id: The FRED series identifier\n",
    "        observation_start: Start date (YYYY-MM-DD)\n",
    "        observation_end: End date (YYYY-MM-DD)\n",
    "        frequency: Aggregation frequency ('d', 'w', 'm', 'q', 'a')\n",
    "        sort_order: 'asc' or 'desc'\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with date and value columns\n",
    "    \"\"\"\n",
    "    url = f\"{FRED_BASE_URL}/series/observations\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"api_key\": FRED_API_KEY,\n",
    "        \"file_type\": \"json\",\n",
    "        \"sort_order\": sort_order\n",
    "    }\n",
    "    \n",
    "    if observation_start:\n",
    "        params[\"observation_start\"] = observation_start\n",
    "    if observation_end:\n",
    "        params[\"observation_end\"] = observation_end\n",
    "    if frequency:\n",
    "        params[\"frequency\"] = frequency\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    observations = data.get(\"observations\", [])\n",
    "    \n",
    "    if not observations:\n",
    "        return pd.DataFrame(columns=[\"date\", \"value\"])\n",
    "    \n",
    "    df = pd.DataFrame(observations)\n",
    "    df = df[[\"date\", \"value\"]]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_multiple_series(\n",
    "    series_dict: dict,\n",
    "    observation_start: str = None,\n",
    "    observation_end: str = None,\n",
    "    latest_only: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch data for multiple series and combine into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        series_dict: Dictionary mapping series_id to friendly name\n",
    "        observation_start: Start date (YYYY-MM-DD)\n",
    "        observation_end: End date (YYYY-MM-DD)\n",
    "        latest_only: If True, only return the most recent observation for each series\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame with all series data\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    run_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    for series_id, friendly_name in series_dict.items():\n",
    "        print(f\"  Fetching {series_id} ({friendly_name})...\")\n",
    "        \n",
    "        try:\n",
    "            df = get_series_observations(\n",
    "                series_id,\n",
    "                observation_start=observation_start,\n",
    "                observation_end=observation_end\n",
    "            )\n",
    "            \n",
    "            if not df.empty:\n",
    "                df[\"run_timestamp\"] = run_timestamp\n",
    "                df[\"series_id\"] = series_id\n",
    "                df[\"series_name\"] = friendly_name\n",
    "                \n",
    "                # Reorder columns to put run_timestamp first\n",
    "                cols = [\"run_timestamp\", \"series_id\", \"series_name\", \"date\", \"value\"]\n",
    "                df = df[cols]\n",
    "                \n",
    "                if latest_only:\n",
    "                    df = df.tail(1)\n",
    "                \n",
    "                all_data.append(df)\n",
    "                print(f\"    ✓ Retrieved {len(df)} observations\")\n",
    "            else:\n",
    "                print(f\"    ⚠ No data available\")\n",
    "                \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"    ✗ Error: {e}\")\n",
    "        \n",
    "        # Be nice to the API - small delay between requests\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, volume_path: str, filename_prefix: str = \"fred_rates\") -> str:\n",
    "    \"\"\"\n",
    "    Save DataFrame to a timestamped CSV file in Databricks Volume.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        volume_path: Path to the Databricks volume\n",
    "        filename_prefix: Prefix for the filename\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved file\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.csv\"\n",
    "    filepath = f\"{volume_path}/{filename}\"\n",
    "    \n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"\\n✓ Data saved to: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for Databricks.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FRED Rate Data Fetcher - Databricks Edition\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nLanding Zone (CSV Files):\")\n",
    "    print(f\"  Rates Volume:    {VOLUME_PATH_RATES}\")\n",
    "    print(f\"  Metadata Volume: {VOLUME_PATH_METADATA}\")\n",
    "    print(f\"\\nRun Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # GET API KEY FROM DATABRICKS SECRETS\n",
    "    # ==========================================================================\n",
    "    try:\n",
    "        # This works in Databricks notebooks\n",
    "        FRED_API_KEY = dbutils.secrets.get(scope=SECRET_SCOPE, key=SECRET_KEY)\n",
    "        print(f\"✓ API key retrieved from secret scope: {SECRET_SCOPE}\")\n",
    "    except NameError:\n",
    "        # Fallback for local testing (dbutils not available)\n",
    "        import os\n",
    "        FRED_API_KEY = os.environ.get(\"FRED_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
    "        print(\"⚠ Running outside Databricks - using environment variable\")\n",
    "    \n",
    "    if not FRED_API_KEY or FRED_API_KEY == \"YOUR_API_KEY_HERE\":\n",
    "        print(\"\\n⚠ ERROR: Please set your FRED API key!\")\n",
    "        print(\"  In Databricks: Store in secrets scope\")\n",
    "        print(\"  Locally: Set FRED_API_KEY environment variable\")\n",
    "        return\n",
    "    \n",
    "    # Make API key available to other functions via global\n",
    "    globals()['FRED_API_KEY'] = FRED_API_KEY\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # VERIFY VOLUME ACCESS (BOTH VOLUMES)\n",
    "    # ==========================================================================\n",
    "    for volume_name, volume_path in [(\"Rates\", VOLUME_PATH_RATES), (\"Metadata\", VOLUME_PATH_METADATA)]:\n",
    "        try:\n",
    "            test_path = f\"{volume_path}/.write_test\"\n",
    "            with open(test_path, 'w') as f:\n",
    "                f.write(\"test\")\n",
    "            import os\n",
    "            os.remove(test_path)\n",
    "            print(f\"✓ {volume_name} volume access verified: {volume_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠ ERROR: Cannot write to {volume_name} volume: {volume_path}\")\n",
    "            print(f\"  Error: {e}\")\n",
    "            print(\"  Please verify the volume exists and you have write permissions.\")\n",
    "            return\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # FETCH SERIES METADATA (DIMENSIONAL INFORMATION)\n",
    "    # ==========================================================================\n",
    "    metadata_df = fetch_series_metadata(RATE_SERIES)\n",
    "    \n",
    "    if not metadata_df.empty:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        metadata_filepath = f\"{VOLUME_PATH_METADATA}/fred_series_metadata_{timestamp}.json\"\n",
    "        # Use JSON format to properly handle special characters and line breaks\n",
    "        metadata_df.to_json(metadata_filepath, orient='records', lines=True)\n",
    "        print(f\"\\n✓ Metadata saved to: {metadata_filepath}\")\n",
    "        \n",
    "        # Display metadata summary\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SERIES METADATA SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        summary_cols = [\"series_id\", \"title\", \"frequency\", \"units_short\", \"last_updated\"]\n",
    "        print(metadata_df[summary_cols].to_string(index=False))\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # FETCH OBSERVATIONS DATA\n",
    "    # ==========================================================================\n",
    "    print(f\"\\nFetching {len(RATE_SERIES)} rate series...\")\n",
    "    if OBSERVATION_START:\n",
    "        print(f\"  Date range: {OBSERVATION_START} to {OBSERVATION_END or 'present'}\")\n",
    "    \n",
    "    # Fetch all series data\n",
    "    df = fetch_multiple_series(\n",
    "        RATE_SERIES,\n",
    "        observation_start=OBSERVATION_START,\n",
    "        observation_end=OBSERVATION_END\n",
    "    )\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"\\n⚠ No data retrieved. Check your API key and series IDs.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nTotal observations retrieved: {len(df)}\")\n",
    "    \n",
    "    # Save full historical data (long format)\n",
    "    save_to_csv(df, VOLUME_PATH_RATES, \"fred_rates_historical\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓ FRED data fetch completed successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATABRICKS JOB CONFIGURATION EXAMPLE (JSON)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Save this as a JSON file and use with Databricks CLI:\n",
    "    databricks jobs create --json @fred_job_config.json\n",
    "\n",
    "Or use the Databricks REST API to create the job programmatically.\n",
    "\n",
    "{\n",
    "    \"name\": \"FRED Rate Data Daily Fetch\",\n",
    "    \"email_notifications\": {\n",
    "        \"on_failure\": [\"your-email@company.com\"],\n",
    "        \"on_success\": []\n",
    "    },\n",
    "    \"webhook_notifications\": {},\n",
    "    \"timeout_seconds\": 0,\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"fetch_fred_rates\",\n",
    "            \"run_if\": \"ALL_SUCCESS\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/Users/your-email@company.com/fred_rates_fetcher\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"job_cluster_key\": \"fred_cluster\",\n",
    "            \"timeout_seconds\": 0,\n",
    "            \"email_notifications\": {}\n",
    "        }\n",
    "    ],\n",
    "    \"job_clusters\": [\n",
    "        {\n",
    "            \"job_cluster_key\": \"fred_cluster\",\n",
    "            \"new_cluster\": {\n",
    "                \"cluster_name\": \"\",\n",
    "                \"spark_version\": \"14.3.x-scala2.12\",\n",
    "                \"spark_conf\": {},\n",
    "                \"node_type_id\": \"i3.xlarge\",\n",
    "                \"num_workers\": 0,\n",
    "                \"spark_env_vars\": {\n",
    "                    \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\"\n",
    "                },\n",
    "                \"enable_elastic_disk\": false,\n",
    "                \"data_security_mode\": \"SINGLE_USER\",\n",
    "                \"runtime_engine\": \"STANDARD\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 6 * * ?\",\n",
    "        \"timezone_id\": \"America/New_York\",\n",
    "        \"pause_status\": \"UNPAUSED\"\n",
    "    },\n",
    "    \"queue\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"format\": \"MULTI_TASK\"\n",
    "}\n",
    "\n",
    "SCHEDULE EXAMPLES (Quartz Cron Format):\n",
    "---------------------------------------\n",
    "\"0 0 6 * * ?\"     = Daily at 6:00 AM\n",
    "\"0 30 7 * * ?\"    = Daily at 7:30 AM\n",
    "\"0 0 6 ? * MON-FRI\" = Weekdays at 6:00 AM\n",
    "\"0 0 */4 * * ?\"   = Every 4 hours\n",
    "\"0 0 6 1 * ?\"     = Monthly on the 1st at 6:00 AM\n",
    "\n",
    "TIMEZONE OPTIONS:\n",
    "-----------------\n",
    "America/New_York    = Eastern Time\n",
    "America/Chicago     = Central Time\n",
    "America/Denver      = Mountain Time\n",
    "America/Los_Angeles = Pacific Time\n",
    "UTC                 = Coordinated Universal Time\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ALTERNATIVE: DELTA TABLE OUTPUT (Optional Enhancement)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "If you prefer to save data as Delta tables instead of CSV files,\n",
    "you can use this function in a Databricks notebook:\n",
    "\n",
    "def save_to_delta_table(df: pd.DataFrame, table_name: str):\n",
    "    '''\n",
    "    Save DataFrame as a Delta table in Unity Catalog.\n",
    "    '''\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    \n",
    "    full_table_name = f\"investments.fred.{table_name}\"\n",
    "    \n",
    "    spark_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(full_table_name)\n",
    "    \n",
    "    print(f\"✓ Data saved to Delta table: {full_table_name}\")\n",
    "\n",
    "# Usage:\n",
    "# save_to_delta_table(df, \"fred_rates_historical\")\n",
    "# save_to_delta_table(metadata_df, \"fred_rates_metadata\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FRED Daily API CAll",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
